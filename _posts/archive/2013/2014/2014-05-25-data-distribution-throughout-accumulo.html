---
layout: post
title: Data Distribution Throughout the Accumulo Cluster
date: '2014-05-25T08:21:00.000-04:00'
author: David Medinets
categories: accumulo
modified_time: '2014-05-25T08:21:26.307-04:00'
blogger_id: tag:blogger.com,1999:blog-3207985.post-2401492234541240778
blogger_orig_url: http://affy.blogspot.com/2014/05/data-distribution-throughout-accumulo.html
year: 2014
theme: accumulo
---

This document answers these questions:<br />
<br />
 * What is a tablet?<br />
 * What is a split point?<br />
 * What is needed before data can be distributed?<br />


<br />
A distributed database typically is thought of as having data spread across multiple servers. But how does the data spread out? That's a question I hope to answer - at least for Accumulo.<br />
<br />
At a high level of abstraction, the concept is simple. If you have two servers, then 50% of the data should go to server one and 50% should go to server two. The examples below give concrete demonstrations of data distribution.<br />
<br />
Accumulo stores information as key-value pairs (or entries). For a visual reference, below is an empty key-value pair. <br />
<br />
<pre>-----------  ---------
| key     |  | value |
-----------  ---------
| [nothing here yet] |
-----------  ---------
</pre><br />
<h2>Tables</h2><br />
A collecton of key-values is called a table. This table is different from one<br />
found in a relational database because there is no schema associated with it.<br />
<br />
<blockquote>What is a Key? See below.</blockquote><br />
Note: Understanding the difference between a relational database and a <br />
key-value database is beyond the scope of this discussion. If you want, you<br />
can think of the "key" in this discussion as a primary key. But, fair warning,<br />
that is a false analogy. One which you'll need to forget as you gain more<br />
proficiency with key-value databases. <br />
<br />
A new Accumulo table has a single unit of storage called a tablet. When created, the tablet is empty. As more entries are inserted into a table, Accumulo may <br />
automatically decide to split the initial tablet into two tablets. As the size <br />
of the table continues to grow, the split operation is repeated. Or you can <br />
specify how the splitting occurs. We'll discuss this further below.<br />
<br />
>Tables have one or more tablets.<br />
<br />
Below is an empty table. For convenience, we'll use 'default' as the name of <br />
the initial tablet.<br />
<br />
<pre>-----------  -----------  ---------
| tablet  |  | key     |  | value |
-----------  -----------  ---------
| default |  | <nothing here yet> |
-----------  -----------  ---------
</pre><br />
Even though the table is empty, it still has a starting key of -infinity and<br />
an ending key of +infinity. All possible data occurs between the two extremes of infinity.<br />
<br />
<pre>  -infinity ==> ALL DATA  <== +infinity.
</pre>

This concept of start and end keys can be shown in our tablet depiction as
well.

<pre>-----------  -----------  ---------
| tablet  |  | key     |  | value |
-----------  -----------  ---------
|      start key: -infinity       |
-----------------------------------
| default |  | <nothing here yet> |
-----------------------------------
|        end key: +infinity       |
-----------  -----------  ---------
</pre>
After inserting three records into a new table, you'll have the following
situaton. Notice that Accumulo always stores keys in lexically sorted order.
So far, the start and end keys have not been changed.

<pre>-----------  -------  ---------
| tablet  |  | key |  | value |
-----------  -------  ---------
| default |  | 01  |  | X     |
| default |  | 03  |  | X     |
| default |  | 05  |  | X     |
-----------  -------  ---------
</pre>
Accumulo stores all entries for a tablet on a single node in the clsuter. Since our
table has only one tablet, the information can't spread beyond one node. In
order to distribute information, you'll need to create more than tablet for
your table.

<blockquote>The tablet's range is still from -infinity to +infinity. That hasn't changed yet.</blockquote>
<h2>Splits</h2>
Now we can introduce the idea of splits. When a tablet is split, one tablet
becomes two. If you want your information to be spread onto three nodes, you'll
need two splits. We'll illustrate this idea.

<blockquote>Split point - the place where one tablet becomes two.</blockquote>
Let's add two split points to see what happens. As the split points are added, new tablets are created.

<h3>Adding Splits</h3>
<h4>First Split</h4>
First, adding split point 02 results in a second tablet being created. It's worth noting that the tablet names are meaningless. Accumulo assigns internal names that you rarely need to know. I picked "A" and "B" because they are easy to read.

<pre>-----------  -------  ---------
| tablet  |  | key |  | value |
-----------  -------  ---------
| A       |  | 01  |  | X     | range: -infinity to 02 (inclusive)
|       split point 02        |
| B       |  | 03  |  | X     | range: 02 (exclusive) to +infinity
| B       |  | 05  |  | X     |
-----------  -------  ---------
</pre>
The split point does not need to exist as an entry. This feature means that you can pre-split a table by simply giving Accumulo a list of split points.

<h4>Tablet Movement</h4>
Before continuing, let's take a small step back to see how tablets are moved between servers. At first, the table resides on one server. This makes sense - one tablet is on one server.

<pre>--------------------------------
| Tablet Server                |
--------------------------------
|                              |
|  -- Tablet ----------------  |
|  | -infinity to +infinity |  |
|  --------------------------  |
|                              |
--------------------------------
</pre>
Then the first split point is added. Now there are two tablets. However, they are still on a single server. And this also makes sense. Thinking about adding a split point to a table with millions of entries. While the two tablets reside on one server, adding a split is just an accounting change.

<pre>-----------------------------------------------------------------------
| Tablet Server                                                       |
-----------------------------------------------------------------------
|                                                                     |
|  -- Tablet ---------------------   -- Tablet ---------------------  |
|  | -infinity to 02 (inclusive) |   | 02 (exclusive) to +infinity |  |
|  -------------------------------   -------------------------------  |
|                                                                     |
-----------------------------------------------------------------------
</pre>
At some future point, Accumulo might move the second tablet to another Tablet Server.

<pre>------------------------------------|  |------------------------------------
| Tablet Server                     |  | Tablet Server                     |
------------------------------------|  |------------------------------------
|                                   |  |                                   |
|  -- Tablet ---------------------  |  |  -- Tablet ---------------------  |
|  | -infinity to 02 (inclusive) |  |  |  | 02 (exclusive) to +infinity |  |
|  -------------------------------  |  |  -------------------------------  |
|                                   |  |                                   |
-------------------------------------  -------------------------------------
</pre>
<h4>Second Split</h4>
You'll wind up with three tablets when a second split point of "04" is added.

<pre>-----------  -------  ---------
| tablet  |  | key |  | value |
-----------  -------  ---------
| A       |  | 01  |  | X     | range: -infinity to 02 (inclusive)
|       split point 02        |
| B       |  | 03  |  | X     | range: 02 (exclusive) to 04 (inclusive)
|       split point 04        |
| C       |  | 05  |  | X     | range: 04 (exclusive) to +infinity
-----------  -------  ---------
</pre>
The table now has three tablets. When enough tablets are created, some process
inside Accumulo moves one or more tablets into different nodes. Once that
happens the data is distributed.

Hopefully, you can now figure out which tablet any specific key inserts into.
For example, key "00" goes into tablet "A".

<pre>-----------  -------  ---------
| tablet  |  | key |  | value |
-----------  -------  ---------
| A       |  | 00  |  | X     | range: -infinity to 02 (inclusive)
| A       |  | 01  |  | X     |
|       split point 02        |
| B       |  | 03  |  | X     | range: 02 (exclusive) to 04 (inclusive)
|       split point 04        |
| C       |  | 05  |  | X     | range: 04 (exclusive) to +infinity
-----------  -------  ---------
</pre>
Internally, the first tablet ("A") as a starting key of -infinity. Any entry
with a key between -infinity and "00" inserts into the first key. The last
tablet has an ending key of +infinity. Therefore any key between "05" and
+infinity inserts into the last tablet.

Accumulo automatically creates split points based on some conditions. For example, if the tablet grows too large. However, that's a whole 'nother conversation.

<h2>What is a Key?</h2>
Plenty of people have described Accumulo's Key layout. Here is the
bare-bones explanation:

<pre>-------------------------------------------------------------------
| row | column family | column qualifier | visibility | timestamp |
-------------------------------------------------------------------
</pre>
These five components, combined, go into the _Key_.

<h2>Using Shards To Split a Row</h2>
Each row resides on a single tablet which can cause a problem if any single row has a few million entries. For example, if your table held all ISBN's using this schema:

<pre>------------------------------------------------
| row | column family | column qualifier       |
------------------------------------------------
| book | 140122317    | Batman: Hush           |
| book | 1401216676   | Batman: A Killing Joke |
</pre>
You can see how the _book_ row would have millions of entries. Potentially causing memory issues inside your TServer. Many people add a _shard_ value to the row to introduce potential split points. With shard values, the above table might look like this:

<pre>---------------------------------------------------
| row    | column family | column qualifier       |
---------------------------------------------------
| book_0 |  140122317    | Batman: Hush           |
| book_5 |  1401216676   | Batman: A Killing Joke |
</pre>
With this style of row values, Accumulo could use book_5 as a split point so that the row are no longer unmanageable. Of course, this technique adds a bit of complexity to the query process. I'll leave the query issue to a future note.

Let's explore how shard values can be generated.

<h3>When an Accumulo table is created</h3>
It may be tempting to have the computers flip a virtual coin to decide which
server to target for each record. In the RDBMS world that procedure works but
in key-value databases, information is stored vertically instead of
horizontally so the coin flip analogy does not work. Let's quickly review why.

<h4>Coin Flip Sharding</h4>
Relational databases spread information across columns (i.e., horizontally). Hopefully, there is in Id value using a synthetic key (SK) and I hope you have them in your data. If not your very first task is to get your DBA's to add  them. Seriously, synthetic keys save you a world of future trouble. Here is a simple relational record.

<pre>|--------------------------------------
| RELATIONAL REPRESENTATION           |
|--------------------------------------
| SK   | First Name | Last Name | Age |
|-------------------------------------|
| 1001 | John       | Kloplick  | 36  |
---------------------------------------
</pre>
Key-value database spread information across several rows using the synthetic key to tie them together. In simplified form, the information is stored in three key-value combinations (or three entries).

<pre>|----------------------------------
| KEY VALUE REPRESENTATION        |
|----------------------------------
| ROW  | CF         | CQ          |
|---------------------------------|
| 1001 | first_name | John        |
| 1001 | last_name  | Kloplick    |
| 1001 | age        | 36          |
-----------------------------------
</pre>
If the coin flip sharding strategy were used the information might look like the following. The potential split point shows that the entries can be spread across two tablets.

<pre>|-------------------------------------
| ROW     | CF         | CQ          |
|------------------------------------|
| 1001_01 | first_name | John        |
| 1001_01 | age        | 36          |
| 1001_02 | last_name  | Kloplick    | <-- potential split point
--------------------------------------
</pre>

To retrieve the information you'd need to scan both servers! This coin flip sharding technique is not going to scale. Imagine information about a person spread over 40 servers. Collating that information would be prohibitively time-consuming.

<h4>HASH + MOD Sharding (using natural key)</h4>
Of course, there is a better sharding strategy to use. You can base the strategy on one of the fields. Get its hash code and then mod it by the number of partitions. Ultimately, this strategy will fail but let's go through the process to see why. Skip to the next section if you already see the problem.

"John".hashCode() is 2314539. Then we can mod that by the number of partitions (or servers) in our cluster. Let's pretend we have 5 servers instead of the two we used earlier for variety. Our key-value entries now look thusly:

<blockquote>2,314,539 modulo 5 = 4</blockquote>
<pre>|-------------------------------------
| ROW     | CF         | CQ          |
|------------------------------------|
| John_04 | first_name | John        |
| John_04 | age        | 36          |
| John_04 | last_name  | Kloplick    |
--------------------------------------
</pre>
<blockquote>Note that the shard value is _not_ related to any specific node. It's just a potential split point for Accumulo.</blockquote>
It's time to look at a specific use case to see if this sharding strategy is sound. What if we need to add a set of friends for John? It's unlikely that the information about John's friends have his first name. But very likely for his synthetic key of 1001 to be there. We can now see choosing the first_name field as the base of the sharding strategy was unwise.

<h4>HASH + MOD Sharding (using synthetic key)</h4>
Using the synthetic key as the basis for the hash provides more continuity between updates. And regardless of how information changes, we'll always put the information in the same shard.

"1001".hashCode() is 1507424. If we use the first prime number less than 1,000 then the shard calculation generates a shard value of 957.

So the key-value information is now:

<blockquote>1,507,424 modulo 997 = 957</blockquote>
<pre>|--------------------------------------
| ROW      | CF         | CQ          |
|-------------------------------------|
| 1001_957 | first_name | John        |
| 1001_957 | age        | 36          |
| 1001_957 | last_name  | Kloplick    |
--------------------------------------
</pre>
Using this technique makes it simple to add a height field.

<pre>|--------------------------------------------
| ROW      | CF               | CQ          |
|-------------------------------------------|
| 1001_957 | height_in_inches | 68          |
---------------------------------------------
</pre>
