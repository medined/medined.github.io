---
layout: post
title: How I Got Apache Spark to Sort Of (Not Really) Work on my PicoCluster of 5
  Raspberry PI
date: '2016-07-05T19:50:00.002-04:00'
author: David Medinets
tags: 
modified_time: '2016-07-05T19:50:32.670-04:00'
blogger_id: tag:blogger.com,1999:blog-3207985.post-4726785065850136962
blogger_orig_url: http://affy.blogspot.com/2016/07/how-i-got-apache-spark-to-sort-of-not.html
---

I've read several blog posts about people running Apache Spark on a Raspberry PI. It didn't seem too hard so I thought I've have a go at it. But the results were disappointing. Bear in mind that I am a Spark novice so some setting is probably. I ran into two issues - memory and heartbeats.<br />
<br />
So, this what I did.<br />
<br />
I based my work on these pages:<br />
<br />
* https://darrenjw2.wordpress.com/2015/04/17/installing-apache-spark-on-a-raspberry-pi-2/<br />
* https://darrenjw2.wordpress.com/2015/04/18/setting-up-a-standalone-apache-spark-cluster-of-raspberry-pi-2/<br />
* http://www.openkb.info/2014/11/memory-settings-for-spark-standalone_27.html<br />
<br />
I created five SD cards according to my previous blog post (see&nbsp;http://affy.blogspot.com/2016/06/how-did-i-prepare-my-picocluster-for.html).<br />
<br />
<span style="font-size: large;">Installation of Apache Spark</span><br />
<br />
* install Oracle Java and Python<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do (ssh -oStrictHostKeyChecking=no -oCheckHostIP=no pirate@pi0${i}.local sudo apt-get install -y oracle-java8-jdk python2.7 &amp;); done</span><br />
<br />
* download Spark<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.2-bin-hadoop2.6.tgz</span><br />
<br />
* Copy Spark to all RPI<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do (scp -q -oStrictHostKeyChecking=no -oCheckHostIP=no spark-1.6.2-bin-hadoop2.6.tgz pirate@pi0${i}.local:. &amp;&amp; echo "Copy complete to pi0${i}" &amp;); done</span><br />
<br />
* Uncompress Spark<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do (ssh -oStrictHostKeyChecking=no -oCheckHostIP=no pirate@pi0${i}.local tar xfz spark-1.6.2-bin-hadoop2.6.tgz &amp;&amp; echo "Uncompress complete to pi0${i}" &amp;); done</span><br />
<br />
* Remove tgz file<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do (ssh -oStrictHostKeyChecking=no -oCheckHostIP=no pirate@pi0${i}.local rm spark-1.6.2-bin-hadoop2.6.tgz); done</span><br />
<br />
* Add the following to your .bashrc file on each RPI. I can't figure out how to put this into a loop.<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">export SPARK_LOCAL_IP="$(ip route get 1 | awk '{print $NF;exit}')"</span><br />
<br />
* Run Standalone Spark Shell<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">ssh -oStrictHostKeyChecking=no -oCheckHostIP=no pirate@pi01.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">cd spark-1.6.2-bin-hadoop2.6</span><br />
<span style="font-family: Courier New, Courier, monospace;">bin/run-example SparkPi 10</span><br />
<span style="font-family: Courier New, Courier, monospace;">bin/spark-shell --master local[4]</span><br />
<span style="font-family: Courier New, Courier, monospace;"># This takes several minutes to display a prompt.</span><br />
<span style="font-family: Courier New, Courier, monospace;"># While the shell is running, visit http://pi01.local:4040/</span><br />
<span style="font-family: Courier New, Courier, monospace;">scala&gt; sc.textFile("README.md").count</span><br />
<span style="font-family: Courier New, Courier, monospace;"># After the job is complete, visit the monitor page.</span><br />
<span style="font-family: Courier New, Courier, monospace;">scala&gt; exit</span><br />
<span style="font-family: Courier New, Courier, monospace;"><br /></span>
* Run PyShark Shell<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">bin/pyspark --master local[4]</span><br />
<span style="font-family: Courier New, Courier, monospace;">&gt;&gt;&gt; sc.textFile("README.md").count()</span><br />
<span style="font-family: Courier New, Courier, monospace;">&gt;&gt;&gt; exit()</span><br />
<br />
<span style="font-size: large;">CLUSTER</span><br />
<br />
Now for the clustering...<br />
<br />
* Enable password-less SSH between nodes<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">ssh -oStrictHostKeyChecking=no -oCheckHostIP=no pirate@pi01.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do avahi-resolve --name pi0${i}.local -4 | awk ' { t = $1; $1 = $2; $2 = t; print; } ' | sudo tee --append /etc/hosts; done</span><br />
<span style="font-family: Courier New, Courier, monospace;">echo "$(ip route get 1 | awk '{print $NF;exit}') $(hostname).local" | sudo tee --append /etc/hosts</span><br />
<span style="font-family: Courier New, Courier, monospace;">ssh-keygen</span><br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 1 5`; do ssh-copy-id pirate@pi0${i}.local; done</span><br />
<br />
* Configure Spark for Cluster<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">cd spark-1.6.2-bin-hadoop2.6/conf</span><br />
<span style="font-family: Courier New, Courier, monospace;"><br /></span>
<span style="font-family: Courier New, Courier, monospace;">create a slaves file with the following contents</span><br />
<span style="font-family: Courier New, Courier, monospace;">pi01.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">pi02.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">pi03.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">pi04.local</span><br />
<span style="font-family: Courier New, Courier, monospace;">pi05.local</span><br />
<span style="font-family: Courier New, Courier, monospace;"><br /></span>
<span style="font-family: Courier New, Courier, monospace;">cp spark-env.sh.template spark-env.sh</span><br />
<span style="font-family: Courier New, Courier, monospace;">In spark-env.sh</span><br />
<span style="font-family: Courier New, Courier, monospace;">&nbsp; Set SPARK_MASTER_IP the results of "ip route get 1 | awk '{print $NF;exit}'"</span><br />
<span style="font-family: Courier New, Courier, monospace;">&nbsp; SPARK_WORKER_MEMORY=512m</span><br />
<br />
* Copy the spark environment script to the other RPI<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">for i in `seq 2 5`; do scp spark-env.sh pirate@pi0${i}.local:spark-1.6.2-bin-hadoop2.6/conf/; done</span><br />
<br />
* Start the cluster<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">cd ..</span><br />
<span style="font-family: Courier New, Courier, monospace;">sbin/start-all.sh</span><br />
<span style="font-family: Courier New, Courier, monospace;"><br /></span>
* Visit the monitor page<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">http://192.168.1.8:8080</span><br />
<br />
And everything is working so far! But ...<br />
<br />
* Start a Spark Shell<br />
<br />
<span style="font-family: Courier New, Courier, monospace;">bin/spark-shell --executor-memory 500m --driver-memory 500m --master spark://pi01.local:7077 --conf spark.executor.heartbeatInterval=45s&nbsp;</span><br />
<br />
And this fails...<br />
<br />
<br />
<br />